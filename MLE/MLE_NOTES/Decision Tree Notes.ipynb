{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00351bd1-459b-4ae8-ad8a-0c3c2fb55492",
   "metadata": {},
   "source": [
    "# üìò Decision Tree ‚Äì Play Tennis Example  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# üå≥ 1. Introduction\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm that makes decisions by splitting data into branches based on conditions.  \n",
    "\n",
    "It works like asking a sequence of **questions** or **if-else statement**, where each question reduces uncertainty and brings us closer to the final decision (Yes/No).\n",
    "\n",
    "---\n",
    "\n",
    "# üß© 2. Key Terms\n",
    "\n",
    "### **1Ô∏è‚É£ Entropy**\n",
    "A measure of impurity (disorder) in the data.\n",
    "\n",
    "- Entropy = 0 ‚Üí Pure (all Yes or all No)  \n",
    "- Entropy = 1 ‚Üí Maximum impurity (mixed equally)\n",
    "\n",
    "$$\n",
    "Entropy(S) = -p_{yes}\\log_2(p_{yes}) - p_{no}\\log_2(p_{no})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Information Gain**\n",
    "\n",
    "Reduction in entropy after splitting on an attribute.\n",
    "\n",
    "$$\n",
    "Gain(S, A) = Entropy(S) - \\sum_{v \\in values(A)} \\frac{|S_v|}{|S|}Entropy(S_v)\n",
    "$$\n",
    "\n",
    "The attribute with **highest Information Gain** becomes the **root node**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Root Node**\n",
    "The first and most important split in a decision tree.  \n",
    "It provides the **highest reduction in impurity**.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä 3. Play Tennis Dataset\n",
    "\n",
    "| Weather  | Temperature | Humidity | Wind   | Play Tennis? |\n",
    "|----------|-------------|----------|--------|--------------|\n",
    "| Sunny    | Hot         | High     | Weak   | No           |\n",
    "| Sunny    | Hot         | High     | Strong | No           |\n",
    "| Overcast | Hot         | High     | Weak   | Yes          |\n",
    "| Rainy    | Mild        | High     | Weak   | Yes          |\n",
    "| Rainy    | Cool        | Normal   | Weak   | Yes          |\n",
    "| Rainy    | Cool        | Normal   | Strong | No           |\n",
    "| Overcast | Cool        | Normal   | Strong | Yes          |\n",
    "| Sunny    | Mild        | High     | Weak   | No           |\n",
    "| Sunny    | Cool        | Normal   | Weak   | Yes          |\n",
    "| Rainy    | Mild        | Normal   | Weak   | Yes          |\n",
    "| Sunny    | Mild        | Normal   | Strong | Yes          |\n",
    "| Overcast | Mild        | High     | Strong | Yes          |\n",
    "| Overcast | Hot         | Normal   | Weak   | Yes          |\n",
    "| Rainy    | Mild        | High     | Strong | No           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9dfa4-35c3-4621-8a52-8b37154e9930",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "%%{init: {\n",
    "    'themeVariables': {\n",
    "        'fontSize': '26px',\n",
    "        'fontFamily': 'Arial',\n",
    "        'nodeSpacing': '60',\n",
    "        'edgeSpacing': '60',\n",
    "        'padding': '25'\n",
    "    },\n",
    "    'flowchart': {\n",
    "        'rankdir': 'LR',    /* LEFT ‚Üí RIGHT */\n",
    "        'curve': 'basis'\n",
    "    }\n",
    "}}%%\n",
    "\n",
    "flowchart LR\n",
    "\n",
    "    %% --- STYLES ---\n",
    "    classDef yes fill:#9ef7b1,stroke:#1b7a34,stroke-width:4px,color:#000,font-size:26px;\n",
    "    classDef no fill:#f79e9e,stroke:#7a1b1b,stroke-width:4px,color:#000,font-size:26px;\n",
    "    classDef header fill:#cce5ff,stroke:#004085,stroke-width:4px,color:#000,font-size:32px;\n",
    "\n",
    "    %% ROOT\n",
    "    A[Weather<br>Groups]:::header\n",
    "\n",
    "    %% WEATHER BRANCHES (HORIZONTAL)\n",
    "    A --> B[Sunny]\n",
    "    A --> C[Overcast]\n",
    "    A --> D[Rainy]\n",
    "\n",
    "    %% SUNNY\n",
    "    B --> B1[Hot ¬∑ High ¬∑ Weak ‚Üí No]:::no\n",
    "    B --> B2[Hot ¬∑ High ¬∑ Strong ‚Üí No]:::no\n",
    "    B --> B3[Mild ¬∑ High ¬∑ Weak ‚Üí No]:::no\n",
    "    B --> B4[Cool ¬∑ Normal ¬∑ Weak ‚Üí Yes]:::yes\n",
    "    B --> B5[Mild ¬∑ Normal ¬∑ Strong ‚Üí Yes]:::yes\n",
    "\n",
    "    %% OVERCAST\n",
    "    C --> C1[Hot ¬∑ High ¬∑ Weak ‚Üí Yes]:::yes\n",
    "    C --> C2[Cool ¬∑ Normal ¬∑ Strong ‚Üí Yes]:::yes\n",
    "    C --> C3[Mild ¬∑ High ¬∑ Strong ‚Üí Yes]:::yes\n",
    "    C --> C4[Hot ¬∑ Normal ¬∑ Weak ‚Üí Yes]:::yes\n",
    "\n",
    "    %% RAINY\n",
    "    D --> D1[Mild ¬∑ High ¬∑ Weak ‚Üí Yes]:::yes\n",
    "    D --> D2[Cool ¬∑ Normal ¬∑ Weak ‚Üí Yes]:::yes\n",
    "    D --> D3[Cool ¬∑ Normal ¬∑ Strong ‚Üí No]:::no\n",
    "    D --> D4[Mild ¬∑ Normal ¬∑ Weak ‚Üí Yes]:::yes\n",
    "    D --> D5[Mild ¬∑ High ¬∑ Strong ‚Üí No]:::no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef3a80-1904-4654-a2fe-219de0a56ebb",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "%%{init: {\n",
    "    'themeVariables': {\n",
    "        'fontSize': '22px',\n",
    "        'fontFamily': 'Arial',\n",
    "        'nodeSpacing': '40',\n",
    "        'edgeSpacing': '40',\n",
    "        'padding': '20'\n",
    "    },\n",
    "    'flowchart': {\n",
    "        'rankdir': 'LR',   /* LEFT ‚Üí RIGHT */\n",
    "        'curve': 'basis'\n",
    "    }\n",
    "}}%%\n",
    "\n",
    "flowchart LR\n",
    "\n",
    "    %% --- STYLES ---\n",
    "    classDef yes fill:#b7f7c6,stroke:#1f7a34,stroke-width:3px,color:#000,font-size:22px;\n",
    "    classDef no fill:#f7c2c2,stroke:#7a1b1b,stroke-width:3px,color:#000,font-size:22px;\n",
    "    classDef split fill:#cce5ff,stroke:#004085,stroke-width:3px,color:#000,font-size:22px;\n",
    "\"\"\n",
    "    %% ROOT\n",
    "    A[Weather]:::split\n",
    "\n",
    "    %% LEVEL 1 SPLITS\n",
    "    A --> B[Sunny]:::split\n",
    "    A --> C[Overcast]:::split\n",
    "    A --> D[Rainy]:::split\n",
    "\n",
    "    %% SUNNY BRANCH\n",
    "    B --> E[Humidity]:::split\n",
    "    E --> F[High ‚Üí No]:::no\n",
    "    E --> G[Normal ‚Üí Yes]:::yes\n",
    "\n",
    "    %% OVERCAST BRANCH (PURE YES)\n",
    "    C --> H[Yes]:::yes\n",
    "\n",
    "    %% RAINY BRANCH\n",
    "    D --> I[Wind]:::split\n",
    "    I --> J[Weak ‚Üí Yes]:::yes\n",
    "    I --> K[Strong ‚Üí No]:::no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ab620-842d-45e8-a569-4be3c74b1233",
   "metadata": {},
   "source": [
    " Rainy    | Cool        | Normal   | Strong | No           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdfa35-3430-4711-ae69-870c406e6d28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üî¢ 4. Entropy of Full Dataset\n",
    "\n",
    "Total entries = 14  \n",
    "Yes = 9  \n",
    "No = 5  \n",
    "\n",
    "$$\n",
    "p_{yes} = \\frac{9}{14}, \\quad p_{no} = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropy(S) = -\\frac{9}{14}\\log_2\\left(\\frac{9}{14}\\right)\n",
    "             -\\frac{5}{14}\\log_2\\left(\\frac{5}{14}\\right)\n",
    "$$\n",
    "\n",
    "**Final Entropy:**  \n",
    "$$\n",
    "Entropy(S) = 0.94\n",
    "$$\n",
    "\n",
    "---\n",
    "# \n",
    "\n",
    "**Dataset summary:** total = 14, Yes = 9, No = 5\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Information Gain formula\n",
    "\n",
    "For an attribute \\(A\\) with possible values \\(v\\):\n",
    "\n",
    "$$\n",
    "Gain(S,A) = Entropy(S) - \\sum_{v \\in values(A)} \\frac{|S_v|}{|S|}Entropy(S_v)\n",
    "$$\n",
    "\n",
    "We compute \\(Entropy(S_v)\\) for each value \\(v\\), then the weighted sum, then the gain.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Gain calculations (step-by-step)\n",
    "\n",
    "### A. **Weather** (values: Sunny, Overcast, Rainy)\n",
    "\n",
    "Counts & entropies:\n",
    "- Sunny: 5 instances (Yes=2, No=3)  \n",
    "  $$Entropy(Sunny) = -\\frac{2}{5}\\log_2\\frac{2}{5}-\\frac{3}{5}\\log_2\\frac{3}{5}\\approx 0.971$$\n",
    "- Overcast: 4 instances (Yes=4, No=0)  \n",
    "  $$Entropy(Overcast) = 0.000$$\n",
    "- Rainy: 5 instances (Yes=3, No=2)  \n",
    "  $$Entropy(Rainy) \\approx 0.971$$\n",
    "\n",
    "Weighted entropy:\n",
    "\n",
    "$$\n",
    "E_{Weather} = \\frac{5}{14}(0.971)+\\frac{4}{14}(0.000)+\\frac{5}{14}(0.971) \\approx 0.694\n",
    "$$\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "$$\n",
    "Gain(S,Weather) = 0.940 - 0.694 \\approx \\mathbf{0.247}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### B. **Temperature** (values: Hot, Mild, Cool)\n",
    "\n",
    "Counts & entropies:\n",
    "- Hot: 4 (Yes=2, No=2)  \n",
    "  $$Entropy(Hot)= -\\frac{2}{4}\\log_2\\frac{2}{4}-\\frac{2}{4}\\log_2\\frac{2}{4}=1.000$$\n",
    "- Mild: 6 (Yes=4, No=2)  \n",
    "  $$Entropy(Mild)\\approx 0.918$$\n",
    "- Cool: 4 (Yes=3, No=1)  \n",
    "  $$Entropy(Cool)\\approx 0.811$$\n",
    "\n",
    "Weighted entropy:\n",
    "\n",
    "$$\n",
    "E_{Temp} = \\frac{4}{14}(1.000)+\\frac{6}{14}(0.918)+\\frac{4}{14}(0.811) \\approx 0.911\n",
    "$$\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "$$\n",
    "Gain(S,Temperature) = 0.940 - 0.911 \\approx \\mathbf{0.029}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### C. **Humidity** (values: High, Normal)\n",
    "\n",
    "Counts & entropies:\n",
    "- High: 7 (Yes=3, No=4)  \n",
    "  $$Entropy(High)\\approx 0.985$$\n",
    "- Normal: 7 (Yes=6, No=1)  \n",
    "  $$Entropy(Normal)\\approx 0.592$$\n",
    "\n",
    "Weighted entropy:\n",
    "\n",
    "$$\n",
    "E_{Humidity} = \\frac{7}{14}(0.985)+\\frac{7}{14}(0.592) \\approx 0.788\n",
    "$$\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "$$\n",
    "Gain(S,Humidity) = 0.940 - 0.788 \\approx \\mathbf{0.152}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### D. **Wind** (values: Weak, Strong)\n",
    "\n",
    "Counts & entropies:\n",
    "- Weak: 8 (Yes=6, No=2)  \n",
    "  $$Entropy(Weak)\\approx 0.811$$\n",
    "- Strong: 6 (Yes=3, No=3)  \n",
    "  $$Entropy(Strong)=1.000$$\n",
    "\n",
    "Weighted entropy:\n",
    "\n",
    "$$\n",
    "E_{Wind} = \\frac{8}{14}(0.811)+\\frac{6}{14}(1.000) \\approx 0.892\n",
    "$$\n",
    "\n",
    "Information Gain:\n",
    "\n",
    "$$\n",
    "Gain(S,Wind) = 0.940 - 0.892 \\approx \\mathbf{0.048}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Final table (rounded)\n",
    "\n",
    "| Attribute    | Weighted Entropy | Information Gain |\n",
    "|--------------|------------------:|-----------------:|\n",
    "| Weather      | 0.694             | **0.247**        |\n",
    "| Humidity     | 0.788             | 0.152            |\n",
    "| Wind         | 0.892             | 0.048            |\n",
    "| Temperature  | 0.911             | 0.029            |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Conclusion ‚Äî Why **Weather** is the root\n",
    "\n",
    "- **Weather** yields the **largest Information Gain (0.247)** among all attributes.  \n",
    "- That means splitting on **Weather** reduces the dataset entropy the most (creates purer child nodes), so it is chosen as the **root node**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Next steps (after root)\n",
    "After choosing Weather as root, the tree is constructed recursively:\n",
    "- For branch **Sunny** ‚Üí compute gains again among remaining features (Humidity, Wind, Temperature) using only Sunny rows ‚Üí choose best split (Humidity in this dataset).\n",
    "- For **Overcast** ‚Üí becomes pure (all Yes) ‚Üí stop.\n",
    "- For **Rainy** ‚Üí compute gains among remaining features ‚Üí choose best split (Wind in this dataset).\n",
    "This process repeats until all leaf nodes are pure or stopping criteria are met.\n",
    "\n",
    "---\n",
    "\n",
    "*\n",
    "\n",
    "# üßÆ 5. Information Gain (Final Results Only)\n",
    "\n",
    "| Attribute    | Information Gain |\n",
    "|--------------|------------------|\n",
    "| **Weather**  | **0.247** |\n",
    "| Humidity     | 0.151 |\n",
    "| Wind         | 0.048 |\n",
    "| Temperature  | 0.029 |\n",
    "\n",
    "### ‚úîÔ∏è Highest IG ‚Üí **Weather**  \n",
    "So, **Weather becomes the Root Node**.\n",
    "\n",
    "---\n",
    "\n",
    "# üå≥ 6. Final Decision Tree (Play Tennis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deda961-0a1b-4f2d-b3e8-1917ed3ccfd0",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "\n",
    "    A[Weather] --> B[Sunny]\n",
    "    A --> C[Overcast]\n",
    "    A --> D[Rainy]\n",
    "\n",
    "    %% Sunny Branch\n",
    "    B --> E[Humidity]\n",
    "    E --> F[High ‚Üí No]\n",
    "    E --> G[Normal ‚Üí Yes]\n",
    "\n",
    "    %% Overcast Branch\n",
    "    C --> H[Yes]\n",
    "\n",
    "    %% Rainy Branch\n",
    "    D --> I[Wind]\n",
    "    I --> J[Weak ‚Üí Yes]\n",
    "    I --> K[Strong ‚Üí No]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a64235-7146-4c56-9de6-f4be1324df79",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üéØ 7. Applications of Decision Trees\n",
    "\n",
    "- Weather prediction  \n",
    "- Medical diagnosis  \n",
    "- Loan approval systems  \n",
    "- Fraud detection  \n",
    "- Customer behavior prediction  \n",
    "- Game AI decision making  \n",
    "- Student performance classification  \n",
    "\n",
    "---\n",
    "\n",
    "# üìù 8. Summary\n",
    "\n",
    "- Decision Trees split data to reduce impurity.  \n",
    "- **Entropy** measures impurity.  \n",
    "- **Information Gain** measures reduction in impurity.  \n",
    "- Attribute with highest IG becomes **root node**.  \n",
    "- For the Play Tennis dataset ‚Üí **Weather** is the root.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46068c79-3167-4f16-944f-26c849054207",
   "metadata": {},
   "source": [
    "# 9. Gini Impurity \n",
    "\n",
    "## üìå What is Gini Impurity?\n",
    "\n",
    "Gini Impurity tells us **how mixed or impure** a node is.\n",
    "\n",
    "### Intuition:\n",
    "- If a node has **only one class** ‚Üí pure ‚Üí Gini = 0  \n",
    "- If a node has **mixed classes** ‚Üí impure ‚Üí Gini > 0  \n",
    "- Higher Gini = worse split  \n",
    "- Lower Gini = better split\n",
    "\n",
    "Decision Tree tries to **reduce Gini** as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Formula\n",
    "\n",
    "For a node with classes and probabilities \\( p_1, p_2, ..., p_k \\):\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "\n",
    "### For binary classification (Yes/No):\n",
    "\n",
    "$$\n",
    "Gini = 1 - (p_{yes}^2 + p_{no}^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üü¶ Example 1: Pure Node  \n",
    "Data: 10 samples ‚Üí all \"Yes\"  \n",
    "- \\( p_{yes}=1 \\)\n",
    "- \\( p_{no}=0 \\)\n",
    "\n",
    "$$\n",
    "Gini = 1 - (1^2 + 0^2) = 0\n",
    "$$\n",
    "\n",
    "‚úî Pure  \n",
    "‚úî No impurity  \n",
    "\n",
    "---\n",
    "\n",
    "## üüß Example 2: Mixed Node  \n",
    "Data: 5 Yes, 5 No  \n",
    "- \\( p_{yes}=0.5 \\)\n",
    "- \\( p_{no}=0.5 \\)\n",
    "\n",
    "$$\n",
    "Gini = 1 - (0.5^2 + 0.5^2) = 1 - (0.25 + 0.25) = 0.5\n",
    "$$\n",
    "\n",
    "This is **maximum impurity** in binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Why is Gini Used?\n",
    "\n",
    "### ‚úî 1. Fast to compute  \n",
    "No logarithms ‚Üí very efficient.\n",
    "\n",
    "### ‚úî 2. Gives very similar results to Entropy  \n",
    "Most of the time, both choose the **same** split.\n",
    "\n",
    "### ‚úî 3. More sensitive to purity  \n",
    "Gini reacts quickly to class mixing.\n",
    "\n",
    "### ‚úî 4. Works well for classification trees  \n",
    "It is the **default criterion** in sklearn:\n",
    "\n",
    "```python\n",
    "DecisionTreeClassifier(criterion=\"gini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8941a-809e-4331-84e5-7ff2575e957c",
   "metadata": {},
   "source": [
    "| criterion    | Meaning                         |\n",
    "| ------------ | ------------------------------- |\n",
    "| `\"gini\"`     | Split based on Gini impurity    |\n",
    "| `\"entropy\"`  | Split based on Information Gain |\n",
    "| `\"log_loss\"` | Uses probabilistic impurity     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12a49a-cf00-4921-8678-a144a9e56d3f",
   "metadata": {},
   "source": [
    "# üå≥ Decision Tree Hyperparameters \n",
    "\n",
    "Decision Trees can easily **overfit**, so we use hyperparameters to control the tree's growth and improve performance.\n",
    "\n",
    "Here are the most important hyperparameters in a Decision Tree Classifier.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ criterion (Impurity Measure)\n",
    "\n",
    "Controls **how splits are chosen**.\n",
    "\n",
    "Options:\n",
    "- `\"gini\"` ‚Üí Gini Impurity (default, faster)\n",
    "- `\"entropy\"` ‚Üí Information Gain (uses log, slower)\n",
    "- `\"log_loss\"` ‚Üí entropy-like, probability-based\n",
    "\n",
    "Example:\n",
    "```python\n",
    "DecisionTreeClassifier(criterion=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ace103-15c3-4866-b60f-975014717594",
   "metadata": {},
   "source": [
    "## 2 What is max_depth?\n",
    "\n",
    "- **max_depth** controls the **maximum number of levels** in a Decision Tree from the **root node** down to the **leaf nodes**.\n",
    "- It is one of the most important hyperparameters because it **directly affects model complexity**:\n",
    "  - **Too high** ‚Üí tree grows very deep ‚Üí may **overfit** training data.\n",
    "  - **Too low** ‚Üí tree is shallow ‚Üí may **underfit** the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4be62-612d-4cc1-94a5-f1f30862a17b",
   "metadata": {},
   "source": [
    "  ```mathematica\n",
    "                 Weather     ‚Üê Depth 1\n",
    "         /         |         \\\n",
    "     Sunny       Rainy     Overcast   ‚Üê Depth 2\n",
    "      /                         \\\n",
    " Humidity                    Windy     ‚Üê Depth 3\n",
    "  /  \\                       /    \\\n",
    "High  Low                Strong  Weak ‚Üê Depth 4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14db114-4c0d-452a-bf62-7e1be805fd80",
   "metadata": {},
   "source": [
    "Controls how deep the tree can grow.\n",
    "\n",
    "Large depth (20): captures all patterns ‚Üí risk of overfitting\n",
    "\n",
    "Small depth (3 or 4): generalizes well ‚Üí reduces overfitting\n",
    "\n",
    "Example\n",
    "\n",
    "If max_depth=2, the model will only split the tree twice ‚Üí simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48a622-c865-4c07-b3b4-898629e74259",
   "metadata": {},
   "source": [
    "## 3. What is min_samples_split?\n",
    "\n",
    "- **min_samples_split** controls the **minimum number of samples (rows)** a node must have **before it can be split**.\n",
    "- If a node has fewer samples than this value, **splitting is not allowed**.\n",
    "- Helps **prevent overfitting** by stopping tiny nodes from being split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534ab05-615f-4983-9814-9336a5c4c0a7",
   "metadata": {},
   "source": [
    "```java\n",
    "                     ROOT NODE\n",
    "                  (5 samples/rows)\n",
    "                      |\n",
    "       ---------------------------------\n",
    "       |              |                |\n",
    "    Sunny           Rainy          Overcast\n",
    "   (2 rows)        (2 rows)         (1 row)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f72a7-ba5d-4e91-ac77-a54e0e5fdbae",
   "metadata": {},
   "source": [
    "- **Default = 2** (can overfit small datasets)  \n",
    "- Use **cross-validation** to tune for your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc44ef4-6d37-41c7-b271-aedbf8085408",
   "metadata": {},
   "source": [
    "## 4. What is min_samples_leaf?\n",
    "\n",
    "- **min_samples_leaf** sets the **minimum number of samples (rows) required in a leaf node**.\n",
    "- Unlike `min_samples_split` which controls **when a node can split**,  \n",
    "  `min_samples_leaf` controls **the size of the final leaf nodes** after splitting.\n",
    "- Helps **prevent tiny, meaningless leaves** that may cause overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2680e-3d32-4582-814d-3bf5be7c16c4",
   "metadata": {},
   "source": [
    "```java\n",
    "                 ROOT NODE (6 samples)\n",
    "                        |\n",
    "         -------------------------------\n",
    "         |              |              |\n",
    "     Sunny (2)       Rainy (3)     Overcast (1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef924e-480d-437b-b9de-3e8ed68e7da3",
   "metadata": {},
   "source": [
    "```yaml\n",
    "- Suppose `min_samples_leaf = 2`\n",
    "- Split of **Overcast node** would create a leaf with 1 sample ‚Üí **not allowed**  \n",
    "- Any split that would create a leaf with **fewer than 2 samples** is rejected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34b9a2-b219-4db6-9515-b2d7cad6cede",
   "metadata": {},
   "source": [
    "## 5. What is max_features?\n",
    "\n",
    "- **max_features** controls **how many features (columns)** the tree can consider **when looking for the best split** at each node.\n",
    "- It introduces **randomness** and can help **reduce overfitting**.\n",
    "- Default = None (all features considered)\n",
    "\n",
    "- Total features = 4 (Weather, Temperature, Humidity, Wind)\n",
    "- Suppose `max_features = 2`\n",
    "  - At each node, the tree **randomly selects 2 features** to consider for splitting.\n",
    "  - Example:\n",
    "    - Node 1: Features selected ‚Üí Weather & Humidity\n",
    "    - Node 2: Features selected ‚Üí Temperature & Wind\n",
    "  - Reduces overfitting by not always using all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a3d96-1072-48d0-bdbf-94a70c6e46d5",
   "metadata": {},
   "source": [
    "\n",
    "- Tree only checks **Weather** and **Humidity** to decide the split.\n",
    "- Other features (Temp & Wind) are **ignored for this node**.\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "- **Introduces randomness** ‚Üí good for **ensemble methods** like Random Forest.\n",
    "- Can **reduce overfitting** by limiting the features considered at each split.\n",
    "- Works differently depending on tree type:\n",
    "  - **DecisionTreeClassifier** / **DecisionTreeRegressor** ‚Üí limits features per node\n",
    "  - **RandomForest** ‚Üí strongly recommended to set max_features < total features\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8b801-5732-4c4e-b843-332440eae900",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. What is max_leaf_nodes?\n",
    "- **max_leaf_nodes** sets the **maximum number of leaf nodes (endpoints)** in the tree.\n",
    "- Helps **control tree complexity** by limiting the number of final decision nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539b4c0-1c15-4e1f-a516-33a575da2279",
   "metadata": {},
   "source": [
    "- Prevents **overfitting** by keeping the tree **simpler**.\n",
    "- Smaller value ‚Üí simpler tree ‚Üí may underfit  \n",
    "- Larger value ‚Üí more leaves ‚Üí may overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1b7bc-4fd5-43d1-a96c-9e2dee2924dd",
   "metadata": {},
   "source": [
    "## 7. What is min_impurity_decrease?\n",
    "- **min_impurity_decrease** sets the **minimum reduction in impurity** required to make a split.\n",
    "- Parent Node: Impurity = 0.8\n",
    "- Split would reduce impurity to 0.78\n",
    " -min_impurity_decrease = 0.05\n",
    "- ‚Üí 0.02 < 0.05 ‚Üí split not allowed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10d975-96ef-478c-ba24-bdd9ecf25bb2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "| Hyperparameter          | Meaning                                      | Analogy / Visualization                     | Effect |\n",
    "|-------------------------|---------------------------------------------|--------------------------------------------|--------|\n",
    "| **max_depth**           | Maximum levels in tree                      | Building height                             | Controls over/underfitting |\n",
    "| **min_samples_split**   | Minimum samples to split a node            | Minimum students to divide class (before)  | Stops tiny nodes from splitting |\n",
    "| **min_samples_leaf**    | Minimum samples in a leaf                   | Minimum team size (after splitting)        | Prevents tiny leaves |\n",
    "| **max_features**        | Maximum features to consider at each split | Textbooks student can check                 | Introduces randomness, reduces overfitting |\n",
    "| **max_leaf_nodes**      | Maximum number of leaf nodes               | Maximum number of final teams               | Limits complexity, prevents overfitting |\n",
    "| **min_impurity_decrease** | Minimum impurity reduction to split       | Only split if improvement is worth it       | Avoids unnecessary weak splits |\n",
    "\n",
    "---\n",
    "\n",
    "## better Tuning\n",
    "\n",
    "- Use **cross-validation** to select the best values.\n",
    "- Combine hyperparameters for **better control over tree complexity**:\n",
    "  - `max_depth` + `min_samples_split` + `min_samples_leaf` ‚Üí control overfitting\n",
    "  - `max_features` + `min_impurity_decrease` ‚Üí improve generalization\n",
    "  - `max_leaf_nodes` ‚Üí simplify final tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b5e38-6946-492f-b309-179633167ddb",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2dd2fe-2e21-47e8-a055-d40f72424278",
   "metadata": {},
   "source": [
    "# Essemble Techinques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b04ab3-6f36-4630-9ad9-3986749c4b0e",
   "metadata": {},
   "source": [
    "# Ensemble Learning (Bagging, Boosting, Gradient Boosting)\n",
    "\n",
    "## What is Ensemble Learning?\n",
    "Ensemble Learning is a technique in machine learning where multiple models (weak learners) are combined to produce a more accurate and stable prediction.\n",
    "\n",
    "**Key idea:** A group of weak learners together forms a strong learner.\n",
    "\n",
    "---\n",
    "\n",
    "# Bagging (Bootstrap Aggregating)\n",
    "\n",
    "## Definition\n",
    "Bagging is an ensemble method where multiple **independent** models are trained on **bootstrapped samples** (sampling with replacement) from the dataset.  \n",
    "Their predictions are then combined.\n",
    "\n",
    "- For classification ‚Üí **Majority Vote**  \n",
    "- For regression ‚Üí **Average**\n",
    "\n",
    "## Goal\n",
    "To reduce **variance** and prevent overfitting.\n",
    "\n",
    "## How Bagging Works\n",
    "1. Create multiple bootstrapped datasets.\n",
    "2. Train one weak learner (usually decision tree) on each dataset.\n",
    "3. Aggregate all predictions using voting or averaging.\n",
    "\n",
    "## Intuition\n",
    "Each model sees slightly different data, produces slightly different results.  \n",
    "Combining them reduces overall error.\n",
    "\n",
    "## Examples\n",
    "- Random Forest  \n",
    "- Bagged Decision Trees\n",
    "\n",
    "---\n",
    "\n",
    "# Boosting\n",
    "\n",
    "## Definition\n",
    "Boosting is a sequential ensemble technique where each new model focuses on **correcting the errors** made by previous models.  \n",
    "Models are combined using **weighted voting** or **weighted averaging**.\n",
    "\n",
    "## Goal\n",
    "To reduce **bias** and convert weak learners into a strong learner.\n",
    "\n",
    "## How Boosting Works\n",
    "1. Train the first model.\n",
    "2. Identify misclassified samples and increase their weights.\n",
    "3. Train the next model focusing on difficult samples.\n",
    "4. Combine all models with weighted contributions.\n",
    "\n",
    "## Intuition\n",
    "Early models fail on hard samples.  \n",
    "Later models focus more on those, improving accuracy gradually.\n",
    "\n",
    "## Examples\n",
    "  \n",
    "- Gradient Boosting  \n",
    "- XGBoost  \n",
    " \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Gradient Boosting (GBM)\n",
    "\n",
    "## Definition\n",
    "Gradient Boosting improves models step-by-step by using the **gradient of the loss function**.  \n",
    "Each new model fits the **residual errors** (actual ‚àí predicted) of the previous model.\n",
    "\n",
    "## Goal\n",
    "To minimize the loss function by learning in the direction of the **negative gradient**.\n",
    "\n",
    "## How Gradient Boosting Works\n",
    "1. Start with a simple model.\n",
    "2. Calculate residuals (errors).\n",
    "3. Train a new weak model to predict these residuals.\n",
    "4. Update predictions using a learning rate.\n",
    "5. Repeat for many steps.\n",
    "\n",
    "## Intuition\n",
    "Each new weak learner adds a small correction.  \n",
    "Many small corrections combine into a powerful model.\n",
    "\n",
    "## Examples\n",
    "- Gradient Boosting Machine (GBM)  \n",
    "- XGBoost  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Summary Table\n",
    "\n",
    "| Technique | Goal | Training Style | Key Idea | Final Output |\n",
    "|----------|------|----------------|----------|--------------|\n",
    "| Bagging | Reduce variance | Parallel | Train on bootstrapped samples | Majority vote / Average |\n",
    "| Boosting | Reduce bias | Sequential | Focus on misclassified samples | Weighted vote |\n",
    "| Gradient Boosting | Minimize loss | Sequential | Learn from residuals using gradients | Sum of weak learners |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37653992-d913-4ffd-a69c-d73be3b88568",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e5710-2afe-458a-9971-682233724c54",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TB\n",
    "\n",
    "    %% INPUT NODE\n",
    "    A[Input Data Sample]:::input\n",
    "\n",
    "    %% PARALLEL MODELS\n",
    "    A --> M1[Model 1\\nTrained on Bootstrapped Sample]\n",
    "    A --> M2[Model 2\\nTrained on Bootstrapped Sample]\n",
    "    A --> M3[Model 3\\nTrained on Bootstrapped Sample]\n",
    "\n",
    "    %% PREDICTIONS\n",
    "    M1 --> P1[Pred1]\n",
    "    M2 --> P2[Pred2]\n",
    "    M3 --> P3[Pred3]\n",
    "\n",
    "    %% CLASSIFICATION PATH\n",
    "    subgraph C1[Bagging for Classification]\n",
    "        P1 --> V1[Majority Voting]\n",
    "        P2 --> V1\n",
    "        P3 --> V1\n",
    "        V1 --> FC[Final Class Prediction]\n",
    "    end\n",
    "\n",
    "    %% REGRESSION PATH\n",
    "    subgraph R1[Bagging for Regression]\n",
    "        P1 --> AVG[Average]\n",
    "        P2 --> AVG\n",
    "        P3 --> AVG\n",
    "        AVG --> FR[Final Regression Value]\n",
    "    end\n",
    "\n",
    "    %% Styling\n",
    "    classDef input fill:#dbeafe,stroke:#1e40af,color:#1e3a8a;\n",
    "    classDef model fill:#e8f5e9,stroke:#2e7d32,color:#1b5e20;\n",
    "    classDef pred fill:#fff3cd,stroke:#ff9800,color:#e65100;\n",
    "    classDef out fill:#fce4ec,stroke:#c2185b,color:#880e4f;\n",
    "\n",
    "    class A input;\n",
    "    class M1,M2,M3 model;\n",
    "    class P1,P2,P3 pred;\n",
    "    class FC,FR out;\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b755f6-1bc3-466d-968c-37ca210f9c15",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad804b11-aec1-41a6-a8b1-d5efe418f1de",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "flowchart TD\n",
    "\n",
    "    %% INPUT\n",
    "    A[Input Training Data] --> M1\n",
    "\n",
    "    %% MODEL 1\n",
    "    M1[Model 1 - Learns Patterns] --> E1\n",
    "    E1[Compute Errors - Wrong Predictions] --> M2\n",
    "\n",
    "    %% MODEL 2\n",
    "    M2[Model 2 - Focuses on Errors] --> E2\n",
    "    E2[Increase Weight for Misclassified Data] --> M3\n",
    "\n",
    "    %% MODEL 3\n",
    "    M3[Model 3 - Learns Hard Cases] --> C\n",
    "\n",
    "    %% COMBINATION\n",
    "    C[Combine All Models - Weighted Sum] --> F\n",
    "\n",
    "    %% FINAL OUTPUT\n",
    "    F[Final Prediction]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfd1b6-8234-4554-a85f-7b24db6ba69f",
   "metadata": {},
   "source": [
    "## Random  Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ed1eb-1093-4e12-a220-dad720cf9c18",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm Introduction\n",
    "\n",
    "Random Forest is a popular supervised machine learning algorithm used for both classification and regression tasks. It belongs to the ensemble learning family, which means it builds multiple decision trees and merges their predictions to improve accuracy and control overfitting.\n",
    "\n",
    "## How Random Forest Works\n",
    "- **Multiple Decision Trees**: Random Forest builds many decision trees during training. Each tree is trained on a random subset of the training data with a random subset of features.\n",
    "- **Feature Randomness**: Unlike a single decision tree that picks the best feature at each split, Random Forest selects features randomly for each split to create diversity among trees.\n",
    "- **Voting/Averaging**: For classification, each tree votes for a class, and the majority vote is chosen as the final prediction. For regression, the average prediction of all trees is taken.\n",
    "- **Reduced Overfitting**: The randomness and averaging process helps Random Forest reduce overfitting common to single decision trees.\n",
    "\n",
    "## Key Advantages\n",
    "- Handles large datasets with higher dimensionality well.\n",
    "- Works effectively with missing data.\n",
    "- Provides feature importance estimates, helping interpret the model.\n",
    "- Generally has higher accuracy than single decision trees.\n",
    "- Robust to overfitting by averaging multiple trees.\n",
    "\n",
    "## Common Uses of Random Forest\n",
    "- **Classification Tasks**: Email spam detection, disease diagnosis, customer segmentation.\n",
    "- **Regression Tasks**: Predicting house prices, stock market trends, or continuous outcomes.\n",
    "- **Feature Selection**: Identifying the most important predictors in the dataset.\n",
    "- **Anomaly Detection**: Detecting outliers and unusual patterns.\n",
    "\n",
    "Random Forest is widely applied in domains like healthcare, finance, marketing, and any field where accurate predictive analysis is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043a205-5b9b-4d22-81fa-8076cd59dee5",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start: Input Dataset] --> B[Create multiple bootstrap samples]\n",
    "    B --> C[For each sample, build a Decision Tree]\n",
    "    C --> D[At each node, select random subset of features]\n",
    "    D --> E[Split node based on best feature]\n",
    "    E --> F[Repeat splitting until stopping criteria]\n",
    "    F --> G[Each tree makes a prediction]\n",
    "    G --> H{Task Type?}\n",
    "    H -- Classification --> I[Majority voting for class]\n",
    "    H -- Regression --> J[Average predicted values]\n",
    "    I --> K[Final Prediction]\n",
    "    J --> K[Final Prediction]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2f00f-6d39-4aa1-bbd0-92968546de59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
