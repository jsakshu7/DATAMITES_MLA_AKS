{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6b3d58-1f8e-444c-bef5-48c5d88b8143",
   "metadata": {},
   "source": [
    "#  Support Vector Machine (SVM) - Introduction\n",
    "\n",
    "## üîπ What is SVM?\n",
    "**Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification** and **regression** tasks.  \n",
    "It is most commonly used for **binary classification** (e.g., yes/no, spam/ham).\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Basic Idea\n",
    "SVM finds the **best boundary (hyperplane)** that separates data points of different classes.  \n",
    "It tries to **maximize the margin** ‚Äî the distance between the hyperplane and the nearest data points from each class.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How It Works\n",
    "1. Plot data points in space (2D or 3D).\n",
    "2. Draw possible lines (or planes) that separate the classes.\n",
    "3. Choose the **line/plane with the maximum margin**.\n",
    "4. For non-linear data, use a **kernel trick** to project data into a higher dimension where it becomes separable.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Key Terms\n",
    "| Term | Description |\n",
    "|------|--------------|\n",
    "| **Hyperplane** | The decision boundary separating classes. |\n",
    "| **Support Vectors** | Data points closest to the hyperplane; they define the margin. |\n",
    "| **Margin** | The distance between the hyperplane and the support vectors. |\n",
    "| **Kernel Trick** | A mathematical method to handle non-linear data by transforming it into higher dimensions. |\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Types of SVM\n",
    "- **Linear SVM** ‚Üí Straight-line decision boundary  \n",
    "- **Non-linear SVM** ‚Üí Uses kernel functions (Polynomial, RBF, Sigmoid) to handle complex data\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Applications\n",
    "- Text & spam classification  \n",
    "- Face and object recognition  \n",
    "- Medical diagnosis (e.g., tumor classification)  \n",
    "- Fraud detection in finance  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d676f-b825-4cbe-a613-9aaa91afce4c",
   "metadata": {},
   "source": [
    "#  Support Vector Machine (SVM) ‚Äì Key Terminologies\n",
    "\n",
    "## 1. **Hyperplane**\n",
    "- The **decision boundary** that separates data points of different classes.  \n",
    "- In **2D**, it‚Äôs a **line**; in **3D**, it‚Äôs a **plane**; in higher dimensions, it‚Äôs a **hyperplane**.  \n",
    "- SVM tries to find the **best hyperplane** that divides the classes with the **maximum margin**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7e1d7-9264-46ac-942b-474ed110b95d",
   "metadata": {},
   "source": [
    "![hyperplane](./hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a097a2-1f2d-46aa-856d-4079f81e1d12",
   "metadata": {},
   "source": [
    "## 2. **Support Vectors**\n",
    "- The **data points that are closest to the hyperplane**.  \n",
    "- These points directly influence the **position and orientation** of the hyperplane.  \n",
    "- If any of these points are moved or removed, the hyperplane will change.  \n",
    "- Hence, they are called **‚Äúsupport‚Äù** vectors ‚Äî they support the boundary.\n",
    "![support_vect](./supportvectors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861639df-a7a8-4ee0-b925-b690f436a7cc",
   "metadata": {},
   "source": [
    "## 3. **Margin**\n",
    "- The **distance between the hyperplane and the nearest support vectors** from each class.  \n",
    "- SVM aims to **maximize this margin** for better separation and generalization.  \n",
    "- Larger margin ‚Üí better model performance and stability.\n",
    "![margin](./margin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cef99b-074b-4df4-b6d7-8e0d4e10f690",
   "metadata": {},
   "source": [
    "## 4. **Kernel Trick**\n",
    "- A **mathematical technique** used to handle **non-linear data**.  \n",
    "- It transforms the input data into a **higher-dimensional space**, where it becomes **linearly separable**.  \n",
    "- Common kernel types:\n",
    "  - **Linear Kernel** ‚Üí Straight-line separation  \n",
    "  - **Polynomial Kernel** ‚Üí Curved separation  \n",
    "  - **RBF (Radial Basis Function)** ‚Üí Circular or complex separation  \n",
    "  - **Sigmoid Kernel** ‚Üí S-shaped decision boundary\n",
    "  ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbd8bf-ff9b-49c1-85cb-837d2e58562f",
   "metadata": {},
   "source": [
    "## 5. **Decision Boundary**\n",
    "- The **line or surface** that separates one class from another based on the SVM model.  \n",
    "- Points on one side belong to one class, and those on the other side belong to the second class.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Margin Width (Distance Between Support Vectors)**\n",
    "- The **total width** between the two margin boundaries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc536f1",
   "metadata": {},
   "source": [
    "![nonlinear](./non_linear_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba03a2",
   "metadata": {},
   "source": [
    "![linear_svm](./Linear_Linear_SVM_(Fails_on_Non-Linear_Data).png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076f145",
   "metadata": {},
   "source": [
    "![rbf_svm](./RBF_RBF_Kernel_SVM_(Handles_Non-Linear_Data).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fbf54",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623905a",
   "metadata": {},
   "source": [
    "## What are Hyperparameters?\n",
    "\n",
    "üëâ Hyperparameters are **settings or control**s that you choose before training your SVM model.\n",
    "They decide how the model learns and **how flexible or strict** it should be while separating data.\n",
    "\n",
    "Think of them like tuning knobs üîß on a machine.\n",
    "\n",
    "\n",
    "### Important SVM Hyperparameters\n",
    "\n",
    "There are 3 main hyperparameters you‚Äôll use often:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5190f04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486b405d",
   "metadata": {},
   "source": [
    "| Hyperparameter | Symbol / Name            | Works With               | Simple Meaning                                                  |\n",
    "| -------------- | ------------------------ | ------------------------ | --------------------------------------------------------------- |\n",
    "| 1Ô∏è‚É£ `C`        | Regularization parameter | All kernels              | Controls how strictly the model tries to classify training data |\n",
    "| 2Ô∏è‚É£ `gamma`    | RBF kernel parameter     | RBF / Polynomial kernels | Controls how far the influence of a single data point goes      |\n",
    "| 3Ô∏è‚É£ `kernel`   | Type of boundary         | All                      | Decides the shape of the boundary (line, curve, etc.)           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e71e8",
   "metadata": {},
   "source": [
    "# SVM Hyperparameters ‚Äî\n",
    "\n",
    "##  1Ô∏è‚É£ `C` ‚Äî The Strictness Controller\n",
    "**Imagine:** You‚Äôre drawing a line to separate red and blue points.\n",
    "\n",
    "- **If `C` is large (e.g. `C = 1000`)** ‚Üí  \n",
    "  The SVM tries *very* hard to correctly classify every training point (even noise).  \n",
    "  üëâ The boundary becomes **wiggly** and can **overfit**.\n",
    "\n",
    "- **If `C` is small (e.g. `C = 0.1`)** ‚Üí  \n",
    "  The SVM allows some mistakes to get a **smoother boundary**.  \n",
    "  üëâ **Better generalization** (less overfitting).\n",
    "\n",
    "**In short:**\n",
    "\n",
    "| C Value      | Effect                                              |\n",
    "|--------------|-----------------------------------------------------|\n",
    "| High (strict)| Perfect training accuracy but may overfit üò¨        |\n",
    "| Low (relaxed)| Allows errors but generalizes better üòé             |\n",
    "\n",
    "---\n",
    "\n",
    "##  2Ô∏è‚É£ `gamma` ‚Äî The Influence Radius\n",
    "*(Used mostly with RBF or polynomial kernels.)*\n",
    "\n",
    "**Imagine:** Each data point creates a **‚Äúbubble of influence‚Äù** around it.\n",
    "\n",
    "- **High `gamma` (Œ≥ large)** ‚Üí  \n",
    "  Each point‚Äôs bubble is **small** ‚Üí model focuses on **nearby points** ‚Üí **very complex** boundary.\n",
    "\n",
    "- **Low `gamma` (Œ≥ small)** ‚Üí  \n",
    "  Each point‚Äôs bubble is **large** ‚Üí model looks at **broader patterns** ‚Üí **smoother** boundary.\n",
    "\n",
    "**In short:**\n",
    "\n",
    "| gamma Value | Effect                          |\n",
    "|-------------|----------------------------------|\n",
    "| High        | Very detailed, may overfit       |\n",
    "| Low         | Smoother, may underfit           |\n",
    "\n",
    "---\n",
    "\n",
    "##  3Ô∏è‚É£ `kernel` ‚Äî The Shape of the Boundary\n",
    "\n",
    "| Kernel  | Meaning              | Shape of Boundary        | Example Use                          |\n",
    "|---------|----------------------|--------------------------|--------------------------------------|\n",
    "| linear  | Straight line        | Linear                   | Simple, fast when data is linear     |\n",
    "| poly    | Polynomial curve     | Curved                   | When relationship is curved          |\n",
    "| rbf     | Radial Basis Function| Circular / complex       | Most common for non-linear data      |\n",
    "| sigmoid | S-shaped             | Neural-net like          | Rarely used                          |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Putting It All Together ‚Äî Example\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=1, kernel='rbf', gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6dc31",
   "metadata": {},
   "source": [
    "![c_gamma](./c_gamma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a65f20",
   "metadata": {},
   "source": [
    "#  Cross Validation \n",
    "\n",
    "---\n",
    "\n",
    "## üí° Definition\n",
    "\n",
    "**Cross-validation** is a method used to check how well a machine learning model performs on **new or unseen data**.\n",
    "\n",
    "It helps us find out if our model is:\n",
    "- **Overfitting** (too focused on training data), or  \n",
    "- **Underfitting** (too simple to learn patterns).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß© How It Works ‚Äî Step by Step (K-Fold Cross Validation)\n",
    "\n",
    "Let‚Äôs take **K = 5** folds.\n",
    "\n",
    "| Fold | Training Data | Testing Data |\n",
    "|------|----------------|---------------|\n",
    "| 1 | Parts 2,3,4,5 | Part 1 |\n",
    "| 2 | Parts 1,3,4,5 | Part 2 |\n",
    "| 3 | Parts 1,2,4,5 | Part 3 |\n",
    "| 4 | Parts 1,2,3,5 | Part 4 |\n",
    "| 5 | Parts 1,2,3,4 | Part 5 |\n",
    "\n",
    "So, the model is trained **5 times**, each time using a different test part.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495776f",
   "metadata": {},
   "source": [
    "\n",
    "üß† After all 5 rounds ‚Üí take the **average accuracy**  \n",
    "‚Üí This gives a **more reliable performance score**.\n",
    "\n",
    "---\n",
    "\n",
    "## üñºÔ∏è Simple Diagram Explanation\n",
    "\n",
    "Imagine your dataset is split into 5 boxes (folds):\n",
    "\n",
    "| Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 |\n",
    "|:------:|:------:|:------:|:------:|:------:|\n",
    "| ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | üß™ |\n",
    "| ‚úÖ | ‚úÖ | ‚úÖ | üß™ | ‚úÖ |\n",
    "| ‚úÖ | ‚úÖ | üß™ | ‚úÖ | ‚úÖ |\n",
    "| ‚úÖ | üß™ | ‚úÖ | ‚úÖ | ‚úÖ |\n",
    "| üß™ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |\n",
    "\n",
    "‚úÖ = used for **training**  \n",
    "üß™ = used for **testing**\n",
    "\n",
    "Each fold gets one turn as the test set.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d67e9",
   "metadata": {},
   "source": [
    "#  What is GridSearchCV?\n",
    "\n",
    "##  Definition\n",
    "**GridSearchCV** (Grid Search Cross-Validation) is a method used to **find the best hyperparameters** for a machine learning model automatically.\n",
    "\n",
    "It tests **different combinations** of parameters (like `C`, `gamma`, `kernel` in SVM) and tells you which one gives the **best accuracy**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0163a985",
   "metadata": {},
   "source": [
    "##  How It Works\n",
    "\n",
    "1. **You choose parameters to test** (a grid of values).  \n",
    "2. **GridSearchCV** trains and tests your model for every combination using **cross-validation**.  \n",
    "3. It returns:\n",
    "   - Best parameters\n",
    "   - Best model\n",
    "   - Best accuracy score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff9adf",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV for Random Forest\n",
    "When we build a Random Forest model, it has many settings called hyperparameters, like:\n",
    "- Number of trees (n_estimators)\n",
    "- Maximum depth of each tree (max_depth)\n",
    "- Number of features to consider when splitting nodes (max_features)\n",
    "- And others...\n",
    "\n",
    "Choosing the right hyperparameters is important to make the model accurate.\n",
    "\n",
    "**RandomizedSearchCV** helps us find the best hyperparameters by:\n",
    "- Randomly picking combinations of these settings.\n",
    "- Testing each combination with parts of the training data (using cross-validation).\n",
    "- Selecting the combination that gives the best model performance (like accuracy).\n",
    "\n",
    "This way, instead of trying every possible combination (which can take a very long time), it tries a fixed number of random combinations, making the search faster but still effective.\n",
    "\n",
    "### Why use RandomizedSearchCV?\n",
    "- It's faster than exhaustive search (GridSearchCV).\n",
    "- It gives good hyperparameter tuning results.\n",
    "- Helps avoid overfitting by using cross-validation.\n",
    "\n",
    "### Basic Usage Steps:\n",
    "1. Define a range for each hyperparameter.\n",
    "2. Let RandomizedSearchCV randomly sample and test combinations.\n",
    "3. Choose the best hyperparameters based on the evaluation.\n",
    "4. Build the final model using these best settings.\n",
    "\n",
    "It is a practical and smart way to improve Random Forest model performance with less time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a59b7-e192-4dfc-b957-9f50529dbb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
